#
# Abnormal Request Location Detection (ARD) properties
#
# Copyright (C) 2016-2017 xAd, Inc.
#

include = command.properties

#------------------
# Review/Override
#------------------
alert.email = science-ops@xad.com
alert.email.priority = 1

# process window
default.dates = L7

# number of mappers
distcp.max.maps = 6

#----------------
# DIRECTORIES
#----------------
proj.root       = /home/xad
proj.name       = ard
proj.home       = $(proj.root)/$(proj.name)
proj.lib.dir    = $(proj.home)/lib
proj.bin.dir    = $(proj.home)/bin
proj.config.dir = $(proj.home)/config
proj.log.dir    = $(proj.home)/log
proj.status.dir = $(proj.home)/status
proj.tmp.dir    = $(proj.home)/tmp
proj.hive.dir   = $(proj.home)/hive
proj.spark.dir  = $(proj.home)/spark
proj.lock.dir   = $(proj.home)/lock
proj.pig.dir    = $(proj.home)/pig
proj.python.dir = $(proj.home)/python


# Jars
share.java.dir  = $(proj.root)/share/java
jar.avro        = $(share.java.dir)/avro-1.7.6.jar
jar.json-simple = $(share.java.dir)/json-simple-1.1.jar
jar.parquet-pig-bundle = $(proj.lib.dir)/parquet-pig-bundle.jar
jar.xad.common  = $(share.java.dir)/xad_common.jar

# Lock
lock.file = $(proj.lock.dir)/lock

#----------------
# HDFS
#----------------
# Main Dirs
hdfs.data.dir  = /data
hdfs.tmp.dir   = /tmp/ard
hdfs.prod.dir  = /prod/ard

# Data
hdfs.data.extract = $(hdfs.data.dir)/extract
hdfs.data.orc = $(hdfs.data.dir)/science_core_ex
s3.data.science_core_ex = $(hadoop.s3)://xad-science/dw/science_core_ex
s3n.data.science_core_ex = $(hadoop.s3n)://xad-science/dw/science_core_ex

# ABD Folders
hdfs.prod.abd = $(hdfs.prod.dir)/ab_req
# FIXME: need to be able to generate from code.
hdfs.prod.empty = $(hdfs.prod.dir)/empty
hdfs.prod.empty.folder = $(hdfs.prod.empty)/folder
hdfs.prod.empty.hour = $(hdfs.prod.empty)/hour

#----------------------------
# Hive Scripts
#-----------------------------
hive.script.ard-gen = $(proj.hive.dir)/ard-gen.hql
hive.script.ard-gen-partition =  $(proj.hive.dir)/add-partitions.hql

proj.hive.tmp.dir    = $(proj.hive.dir)/tmp

#----------------------------
# Spark Scripts
#----------------------------
spark.script.model = $(proj.spark.dir)/ard_model.py
spark.script.join = $(proj.spark.dir)/ard_join.py
spark.script.orc = $(proj.spark.dir)/ard_orc.py

spark.default.databricks = com.databricks:spark-avro_2.11:3.2.0
spark.default.driver_memory = 4g

# Process Memory
spark.process.executor_memory = 6g
spark.process.num_executors = 12
spark.process.executor_cores = 1

# Join Memory
spark.join.executor_memory = 8g
spark.join.num_executors = 16
spark.join.executor_cores = 1

# ORC Memory
spark.orc.executor_memory = 6g
spark.orc.num_executors = 10
spark.orc.executor_cores = 1

# Country Specific
spark.process.num_executors.us = 24
spark.join.num_executors.us = 32
spark.orc.executor_memory.us = 8g
spark.orc.num_executors.us = 20

spark.process.num_executors.gb = 12
spark.join.num_executors.gb = 16
spark.orc.num_executors.gb = 12


#----------------------------
# Default/Common Properties
#-----------------------------
default.countries   = us gb in de ca fr cn it es jp au se no nl lu fi dk ch at be nz
default.logtypes    = exchange display euwest1
default.logtypes.cn = cnnorth1
default.logtypes.jp = exchange display euwest1 apnortheast1
default.logtypes.au = exchange display euwest1 apnortheast1
default.logtypes.nz = exchange display euwest1 apnortheast1

#--------------------
# Science Core
#--------------------
science_core.fill.folders = fill nf
science_core.nf.folders = nf
science_core.sl.folders = tll pos rest

#--------------------
# Common Properties
#--------------------
ard.file.success = _SUCCESS
ard.process.window = L7
ard.model.countries = us gb de

#--------------------------
# Publisher ETL Properties
#--------------------------
pub.data.prefix.s3 = s3://xad-science/forecast_etl/data
pub.data.prefix.hdfs = /data/forecast_etl
pub.events      = hourly_summary
pub.keep.window = 30
pub.dates       = L2-1
pub.tmp.dir     = ${hdfs.tmp.dir}

#---------
# Python
#---------
python.cmd = /opt/anaconda/bin/python2.7
pyspark.script.split_poi = $(proj.python.dir)/split_poi.py
python.script.ard_mapper = $(proj.python.dir)/ard_mapper_orc.py
python.script.ard_reducer = $(proj.python.dir)/ard_reducer_orc.py

#--------------
# Hive Server
#--------------
hiveserver.uri = jdbc:hive2://ip-172-17-31-58.ec2.internal:2181,ip-172-17-30-155.ec2.internal:2181,ip-172-17-28-34.ec2.internal:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
hive.command = HADOOP_CLIENT_OPTS="-Djline.terminal=jline.UnsupportedTerminal" beeline

#------------
# ARD Gen
#------------
ard.input.table = science_core_hrly
#ard.tmp.table = xianglingmeng.ard_orc
ard.output.table = science_core_orc

ard.default.queue = ard
#ard.default.tmp.schema = xianglingmeng.tmp
#ard.default.join.schema = xianglingmeng.join

#--------------------------
# SmartLocation Centroids
#--------------------------
# FIXME
sl.proj.root = /user/xad
sl.output.dir = $(sl.proj.root)/smartlocation/output
sl.hourly.folder = update_hrly
sl.hourly.ll.folder = $(sl.hourly.folder)/raw_hrly_ll_centroids
sl.hourly.ip.folder = $(sl.hourly.folder)/raw_hrly_94_caches

# Control the number delays
sl.hourly.delay.min = 6
sl.hourly.delay.max = 24


#-------------------
# Status Log
#-------------------
# Common
status_log_local.table = status_log
status_log_local.db.type = mysql
status_log_local.db.conn.mysql.user = etl
status_log_local.db.conn.mysql.password = etlxaddb
status_log_local.db.conn.mysql.dbname = xad_etl
# SCI4
status_log_local.db.conn.mysql.host = db.ambari.mgmt.xad.com
status_log_local.db.conn.mysql.port = 3336
# MV1
#status_log_local.db.conn.mysql.host = nn02.corp.xad.com
#status_log_local.db.conn.mysql.port = 3306

#-------
# Keys
#-------
# - orc
status_log_local.key.science_core_orc = science_core_orc
status_log_local.key.add_partition = add_partition
status_log_local.tag.daily = DAILY
# - s3put 
status_log_local.key.science_core_s3put = science_core_s3put
status_log_local.tag.s3put = 
status_log_local.tag.s3put.daily = DAILY
# - s3hive 
status_log_local.key.science_core_s3hive = science_core_s3hive
status_log_local.tag.s3hive =
status_log_local.tag.s3hive.daily = DAILY
# - fixrep (fix repeated loc_score folders)
status_log_local.key.fixrep = science_core_fixrep

# - Smart Location (foreign)
status_log_local.key.sl.centroid = sl_centroid
status_log_local.tag.sl.hourly = hourly
status_log_local.tag.sl.daily  = DAILY


##---------------
## File Status
##---------------
proj.status.dir = $(proj.log.dir)/status

#------------
# S3
#------------
s3.access_key = AKIAJQ5GHKKUOYMQLZSQ
s3.secret_key = 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
hadoop.s3n = s3n
hadoop.s3a = s3a
hadoop.s3 = $(hadoop.s3a)

#-----------------
# Regeneration
#-----------------
orc.data.hdfs.re = /data/science_core_ex_new

#-----------------
# Local Override
#-----------------
include = local-share.properties [optional]
include = local.properties [optional]
include = dev.properties [optional]

