#
# Abnormal Request Location Detection (ARD) properties
#
# Copyright (C) 2016-2017 xAd, Inc.
#

include = command.properties

#------------------
# Review/Override
#------------------
alert.email = science-ops@xad.com
alert.email.priority = 1

# process window
default.dates = L7

# number of mappers
distcp.max.maps = 6

#----------------
# DIRECTORIES
#----------------
proj.root       = /home/xad
proj.name       = ard
proj.home       = $(proj.root)/$(proj.name)
proj.lib.dir    = $(proj.home)/lib
proj.bin.dir    = $(proj.home)/bin
proj.config.dir = $(proj.home)/config
proj.log.dir    = $(proj.home)/log
proj.status.dir = $(proj.home)/status
proj.tmp.dir    = $(proj.home)/tmp
proj.hive.dir   = $(proj.home)/hive
proj.spark.dir  = $(proj.home)/spark
proj.lock.dir   = $(proj.home)/lock
proj.pig.dir    = $(proj.home)/pig
proj.python.dir = $(proj.home)/python


# Jars
share.java.dir  = $(proj.root)/share/java
jar.avro        = $(share.java.dir)/avro-1.7.6.jar
jar.json-simple = $(share.java.dir)/json-simple-1.1.jar
jar.parquet-pig-bundle = $(proj.lib.dir)/parquet-pig-bundle.jar
jar.xad.common  = $(share.java.dir)/xad_common.jar

# Lock
lock.file = $(proj.lock.dir)/lock

#----------------
# HDFS
#----------------
hdfs.data.dir  = /data
#hdfs.user.dir  = /user
hdfs.tmp.dir   = /tmp/ard
hdfs.prod.dir  = /prod/ard

#hdfs.model.dir = /user/xianglingmeng/ard/ard_model_files
#hdfs.model.mapper.orc.dir = $(hdfs.model.dir)/ard_mapper_orc.py
#hdfs.model.reducer.orc.dir = $(hdfs.model.dir)/ard_reducer_orc.py
#hdfs.model.mapper.dir = $(hdfs.model.dir)/mapper.py
#hdfs.model.reducer.dir = $(hdfs.model.dir)/reducer.py


#----------------------------
# Hive Scripts
#-----------------------------
hive.script.ard-gen = $(proj.hive.dir)/ard-gen.hql
hive.script.ard-gen-partition =  $(proj.hive.dir)/add-partitions.hql

proj.hive.tmp.dir    = $(proj.hive.dir)/tmp

#----------------------------
# Spark Scripts
#----------------------------
spark.script.process = $(proj.spark.dir)/ard_process.py
spark.script.join = $(proj.spark.dir)/ard_join.py
spark.script.orc = $(proj.spark.dir)/ard_orc.py

spark.default.driver_memory = 4g
spark.process.executor_memory = 4g
spark.process.num_executors = 32
spark.process.executor_cores =1
spark.join.executor_memory = 4g
spark.join.num_executors = 32
spark.join.executor_cores =1
spark.default.databricks = com.databricks:spark-avro_2.11:3.2.0

spark.process.executor_memory.gb = 4g
spark.process.num_executors.gb = 16
spark.process.executor_cores.gb =1
spark.join.executor_memory.gb = 4g
spark.join.num_executors.gb = 16
spark.join.executor_cores.gb =1

spark.orc.executor_memory.other = 4g
spark.orc.num_executors.other = 12
spark.orc.executor_cores.other =1


#----------------------------
# Default/Common Properties
#-----------------------------
default.countries   = us gb in de ca fr cn it es jp au se no nl lu fi dk ch at be nz
default.logtypes    = exchange display euwest1
default.logtypes.cn = cnnorth1
default.logtypes.jp = exchange display euwest1 apnortheast1
default.logtypes.au = exchange display euwest1 apnortheast1
default.logtypes.nz = exchange display euwest1 apnortheast1

#--------------------
# Science Core
#--------------------
science_core.fill.folders = fill nf
science_core.sl.folders = tll pos rest
extract.data.prefix.hdfs = /data/extract
hdfs.data.orc = /data/science_core_ex
hdfs.prod.abd = $(hdfs.prod.dir)/ab_req
orc.data.hdfs = /data/science_core_ex

#--------------------
# Common Properties
#--------------------
ard.file.success = _SUCCESS
ard.process.window = L7

#--------------------------
# Publisher ETL Properties
#--------------------------
pub.data.prefix.s3 = s3://xad-science/forecast_etl/data
pub.data.prefix.hdfs = /data/forecast_etl
pub.events      = hourly_summary
pub.keep.window = 30
pub.dates       = L2-1
pub.tmp.dir     = ${hdfs.tmp.dir}

#---------
# Python
#---------
python.cmd = /opt/anaconda/bin/python2.7
pyspark.script.split_poi = $(proj.python.dir)/split_poi.py
python.script.ard_mapper = $(proj.python.dir)/ard_mapper_orc.py
python.script.ard_reducer = $(proj.python.dir)/ard_reducer_orc.py

#--------------
# Hive Server
#--------------
hiveserver.uri = jdbc:hive2://ip-172-17-31-58.ec2.internal:2181,ip-172-17-30-155.ec2.internal:2181,ip-172-17-28-34.ec2.internal:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2

#------------
# ARD Gen
#------------
ard.input.table = science_core_hrly
#ard.tmp.table = xianglingmeng.ard_orc
ard.output.table = science_core_orc


ard.default.queue = ard
#ard.default.tmp.schema = xianglingmeng.tmp
#ard.default.join.schema = xianglingmeng.join

#-------------------
# Local Status Log
#-------------------
status_log_local.table = status_log
status_log_local.db.type = mysql
status_log_local.db.conn.mysql.user = etl
status_log_local.db.conn.mysql.password = etlxaddb
status_log_local.db.conn.mysql.dbname = xad_etl
# SCI4
status_log_local.db.conn.mysql.host = db.ambari.mgmt.xad.com
status_log_local.db.conn.mysql.port = 3336

# MV1
#status_log_local.db.conn.mysql.host = nn02.corp.xad.com
#status_log_local.db.conn.mysql.port = 3306

# Status Log Keys
# houry key = science_core_x/us/exchange       2017 2 7 15
# daily key = science_core_x/us/exchange/DAILY 2017 2 7 NULL
status_log_local.key.science_core_x = science_core_orc
status_log_local.key.add_partition = add_partition
status_log_local.tag.daily = DAILY

##---------------
## File Status
##---------------
proj.status.dir = $(proj.log.dir)/status

#------------
# S3
#------------
s3.access_key = AKIAJQ5GHKKUOYMQLZSQ
s3.secret_key = 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
hadoop.s3n = s3n
hadoop.s3a = s3a
hadoop.s3 = $(hadoop.s3n)

#-----------------
# Local Override
#-----------------
include = local-share.properties [optional]
include = local.properties [optional]
include = dev.properties [optional]

#-----------------
# Regeneration
#-----------------
orc.data.hdfs.re = /data/science_core_ex_new

