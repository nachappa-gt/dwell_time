#
# ard properties
#
# Copyright (C) 2014,2016 xAd, Inc.
#

include = command.properties

#------------------
# Review/Override
#------------------
alert.email = xiangling.meng@xad.com
alert.email.priority = 1

# process window: last two days
default.dates = L2-1

# number of mappers
distcp.max.maps = 6

#----------------
# DIRECTORIES
#----------------
proj.root       = /home/xad
proj.name       = ard
proj.home       = $(proj.root)/$(proj.name)
proj.lib.dir    = $(proj.home)/lib
proj.bin.dir    = $(proj.home)/bin
proj.config.dir = $(proj.home)/config
proj.log.dir    = $(proj.home)/log
proj.status.dir = $(proj.home)/status
proj.tmp.dir    = $(proj.home)/tmp
proj.hive.dir   = $(proj.home)/hive
proj.lock.dir   = $(proj.home)/lock
proj.pig.dir    = $(proj.home)/pig
proj.python.dir = $(proj.home)/python

# Jars
share.java.dir  = $(proj.root)/share/java
jar.avro        = $(share.java.dir)/avro-1.7.6.jar
jar.json-simple = $(share.java.dir)/json-simple-1.1.jar
jar.parquet-pig-bundle = $(proj.lib.dir)/parquet-pig-bundle.jar
jar.xad.common  = $(share.java.dir)/xad_common.jar

# Lock
lock.file = $(proj.lock.dir)/lock

#----------------
# HDFS
#----------------
hdfs.root.local = $(proj.root)/hdfs
hdfs.root.cluster =
hdfs.root      = $(hdfs.root.local)
hdfs.share.dir = $(hdfs.root)/share
hdfs.data.dir  = $(hdfs.root)/data
hdfs.tmp.dir   = /tmp/ard


#----------------------------
# Hive Scripts
#-----------------------------
hive.script.ard-gen = $(proj.hive.dir)/ard-gen.hql

#----------------------------
# Default/Common Properties
#-----------------------------
default.countries   = us gb in de ca fr cn it es jp
default.logtypes    = display exchange euwest1
default.logtypes.cn = cnnorth1
default.logtypes.us = display exchange euwest1 search
default.logtypes.jp = display exchange euwest1 apnortheast1

#--------------------
# Science Core
#--------------------
science_core.fill.folders = fill nf
science_core.sl.folders = tll pos rest

#--------------------
# Common Properties
#--------------------
ard.file.success = _SUCCESS
ard.process.window = L7

#--------------------------
# Publisher ETL Properties
#--------------------------
pub.data.prefix.s3 = s3://xad-science/forecast_etl/data
pub.data.prefix.hdfs = /data/forecast_etl
pub.events      = hourly_summary
pub.keep.window = 30
pub.dates       = L2-1
pub.tmp.dir     = ${hdfs.tmp.dir}

#---------
# Python
#---------
python.cmd = /opt/anaconda/bin/python2.7
pyspark.script.split_poi = $(proj.python.dir)/split_poi.py

#--------
# Spark
#--------
spark.driver.memory = 2G
spark.executor.memory = 4G
spark.num.executors = 8

#---------------------
# Science Core (AVRO)
#---------------------
# Data location in HDFS
extract.data.prefix.hdfs = /data/extract
# MysQL status prefix for logs in S3
extract.status_log.prefix = /enigma/extract
# MysQL status prefix for logs in HDFS
extract.download.status_log.prefix = sci3:science_core_avro
extract.dates = L1
extract.keep.window = 60
extract.keep.window.nofill = 60
extract.log.dir = $(proj.log.dir)/extract

# Status Log
#------------
# MySQL
#status_log.db.conn.mysql.user     = $(xadcms.mysql.user)
#status_log.db.conn.mysql.password = $(xadcms.mysql.passwd)
#status_log.db.conn.mysql.host     = $(xadcms.mysql.master.host)
#status_log.db.conn.mysql.port     = $(xadcms.mysql.master.port)
#status_log.db.conn.mysql.dbname   = enigma_etl_hd2
#status_log.db.type  = mysql
#status_log.table    = status_log

## Local
proj.status.dir = $(proj.log.dir)/status

#------------
# S3
#------------
s3.access_key = AKIAJQ5GHKKUOYMQLZSQ
s3.secret_key = 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
hadoop.s3n = s3n
hadoop.s3a = s3a
hadoop.s3 = $(hadoop.s3n)

#-----------------
# Local Override
#-----------------
include = local-share.properties [optional]
include = local.properties [optional]
include = dev.properties [optional]

