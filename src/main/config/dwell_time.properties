#
# Abnormal Request Location Detection (ARD) properties
#
# Copyright (C) 2016-2017 xAd, Inc.
#

include = command.properties

#------------------
# Review/Override
#------------------
###alert.email = science-ops@xad.com
alert.email = nachappa.ponnappa@groudtruth.com
alert.email.priority = 1

# process window
default.dates = L7

# number of mappers
distcp.max.maps = 6

#----------------
# DIRECTORIES
#----------------
proj.root       = /home/xad
proj.name       = dwell_time
proj.home       = $(proj.root)/$(proj.name)
proj.lib.dir    = $(proj.home)/lib
proj.bin.dir    = $(proj.home)/bin
proj.config.dir = $(proj.home)/config
proj.log.dir    = $(proj.home)/log
proj.status.dir = $(proj.home)/status
proj.tmp.dir    = $(proj.home)/tmp
proj.hive.dir   = $(proj.home)/hive
proj.spark.dir  = $(proj.home)/spark
proj.lock.dir   = $(proj.home)/lock
proj.pig.dir    = $(proj.home)/pig
proj.python.dir = $(proj.home)/python


# Jars
share.java.dir  = $(proj.root)/share/java
jar.avro        = $(share.java.dir)/avro-1.7.6.jar
jar.json-simple = $(share.java.dir)/json-simple-1.1.jar
jar.parquet-pig-bundle = $(proj.lib.dir)/parquet-pig-bundle.jar
jar.xad.common  = $(share.java.dir)/xad_common.jar

# Lock
lock.file = $(proj.lock.dir)/lock

#----------------
# HDFS
#----------------
# Main Dirs
hdfs.data.dir  = /data

#----------------------------
# Hive Scripts
#-----------------------------
hive.script.ard-gen = $(proj.hive.dir)/ard-gen.hql
hive.script.ard-gen-partition =  $(proj.hive.dir)/add-partitions.hql

proj.hive.tmp.dir    = $(proj.hive.dir)/tmp

#----------------------------
# Spark Scripts
#----------------------------
spark.script.model = $(proj.spark.dir)/visit_model.py
spark.default.driver_memory = 4g

# Process Memory
spark.process.executor_memory = 14g
spark.process.num_executors = 48
spark.process.executor_cores = 1

# Join Memory
spark.join.executor_memory = 8g
spark.join.num_executors = 16
spark.join.executor_cores = 1

# ORC Memory
spark.orc.executor_memory = 6g
spark.orc.num_executors = 10
spark.orc.executor_cores = 1

# Country Specific
spark.process.num_executors.us = 24
spark.join.num_executors.us = 32
spark.orc.executor_memory.us = 8g
spark.orc.num_executors.us = 20

#spark.input.dir = tmp/ade/dwell_time/interim/
data.hql.output.dir = /prod/dwell_time/data/dwell_time_input/daily/
data.output.dir = /prod/dwell_time/data/dwell_time/daily/

#----------------------------
# Default/Common Properties
#-----------------------------
default.countries   = us
default.logtypes    = exchange display euwest1

#--------------------
# Common Properties
#--------------------
ard.file.success = _SUCCESS
ard.process.window = L7
ard.model.countries = us


#--------------
# Hive Server
#--------------
###hiveserver.uri = jdbc:hive2://ip-172-17-31-58.ec2.internal:2181,ip-172-17-30-155.ec2.internal:2181,ip-172-17-28-34.ec2.internal:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
hiveserver.uri = jdbc:hive2://nn01.corp.xad.com:2181,nn02.corp.xad.com:2181,nn03.corp.xad.com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2
hive.command = HADOOP_CLIENT_OPTS="-Djline.terminal=jline.UnsupportedTerminal" beeline
hive.version = 2


dwell_time.output.table = science_core_orc

dwell_time.default.queue = sv_misc
#dwell_time.default.queue = ard


#-------------------
# Status Log
#-------------------
# Common
status_log_local.table = status_log
status_log_local.db.type = mysql
status_log_local.db.conn.mysql.user = etl
status_log_local.db.conn.mysql.password = etlxaddb
status_log_local.db.conn.mysql.dbname = xad_etl
# SCI4
###status_log_local.db.conn.mysql.host = db.ambari.mgmt.xad.com
###status_log_local.db.conn.mysql.port = 3336
# MV1
status_log_local.db.conn.mysql.host = nn02.corp.xad.com
status_log_local.db.conn.mysql.port = 3306

#-------
# Keys
#-------
# - orc
status_log_local.key.science_core_orc = science_core_orc
#status_log_local.key.add_partition = add_partition
status_log_local.tag.daily = DAILY

# - New Code
status_log_local.key.dwell_time_prepare = dwell_time_prepare
status_log_local.tag.prepare.daily = DAILY

status_log_local.key.dwell_time_process = dwell_time_process
status_log_local.tag.process.daily = DAILY


##---------------
## File Status
##---------------
proj.status.dir = $(proj.log.dir)/status

#-----------------
# Local Override
#-----------------
include = local-share.properties [optional]
include = local.properties [optional]
include = dev.properties [optional]

