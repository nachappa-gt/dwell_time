#
# ard properties
#
# Copyright (C) 2014,2016 xAd, Inc.
#

include = command.properties

#------------------
# Review/Override
#------------------
alert.email = xiangling.meng@xad.com
alert.email.priority = 1

# process window: last two days
default.dates = L2-1

# number of mappers
distcp.max.maps = 6

#----------------
# DIRECTORIES
#----------------
proj.root       = /home/xad
proj.name       = ard
proj.home       = $(proj.root)/$(proj.name)
proj.lib.dir    = $(proj.home)/lib
proj.bin.dir    = $(proj.home)/bin
proj.config.dir = $(proj.home)/config
proj.log.dir    = $(proj.home)/log
proj.status.dir = $(proj.home)/status
proj.tmp.dir    = $(proj.home)/tmp
proj.hive.dir   = $(proj.home)/hive
proj.spark.dir  = $(proj.home)/spark
proj.lock.dir   = $(proj.home)/lock
proj.pig.dir    = $(proj.home)/pig
proj.python.dir = $(proj.home)/python


# Jars
share.java.dir  = $(proj.root)/share/java
jar.avro        = $(share.java.dir)/avro-1.7.6.jar
jar.json-simple = $(share.java.dir)/json-simple-1.1.jar
jar.parquet-pig-bundle = $(proj.lib.dir)/parquet-pig-bundle.jar
jar.xad.common  = $(share.java.dir)/xad_common.jar

# Lock
lock.file = $(proj.lock.dir)/lock

#----------------
# HDFS
#----------------
hdfs.root.local = $(proj.root)/hdfs
hdfs.root.cluster =
hdfs.root      = $(hdfs.root.local)
hdfs.share.dir = $(hdfs.root)/share
hdfs.data.dir  = $(hdfs.root)/data
hdfs.tmp.dir   = /tmp/ard
hdfs.user.dir = $(hdfs.root)/user

hdfs.model.dir = /user/xianglingmeng/ard/ard_model_files
hdfs.model.mapper.orc.dir = $(hdfs.model.dir)/ard_mapper_orc.py
hdfs.model.reducer.orc.dir = $(hdfs.model.dir)/ard_reducer_orc.py
hdfs.model.mapper.dir = $(hdfs.model.dir)/mapper.py
hdfs.model.reducer.dir = $(hdfs.model.dir)/reducer.py


#----------------------------
# Hive Scripts
#-----------------------------
hive.script.ard-gen = $(proj.hive.dir)/ard-gen.hql
hive.script.ard-gen-partition =  $(proj.hive.dir)/add-partitions.hql

proj.hive.tmp.dir    = $(proj.hive.dir)/tmp

#----------------------------
# Spark Scripts
#----------------------------
spark.script.process = $(proj.spark.dir)/ard_process.py

spark.default.driver_memory = 16g
spark.default.executor_memory = 16g
spark.default.num_executors = 126
spark.default.executor_cores = 5
spark.default.databricks = com.databricks:spark-avro_2.11:3.2.0

#----------------------------
# Default/Common Properties
#-----------------------------
default.countries   = us gb in de ca fr cn it es jp
default.logtypes    = display exchange euwest1
default.logtypes.cn = cnnorth1
default.logtypes.us = display exchange euwest1 search
default.logtypes.jp = display exchange euwest1 apnortheast1

#--------------------
# Science Core
#--------------------
science_core.fill.folders = fill nf
science_core.sl.folders = tll pos rest
extract.data.prefix.hdfs = /data/extract
orc.data.hdfs = /user/xad/tmp/ard/science_core_ex


#--------------------
# Common Properties
#--------------------
ard.file.success = _SUCCESS
ard.process.window = L7

#--------------------------
# Publisher ETL Properties
#--------------------------
pub.data.prefix.s3 = s3://xad-science/forecast_etl/data
pub.data.prefix.hdfs = /data/forecast_etl
pub.events      = hourly_summary
pub.keep.window = 30
pub.dates       = L2-1
pub.tmp.dir     = ${hdfs.tmp.dir}

#---------
# Python
#---------
python.cmd = /opt/anaconda/bin/python2.7
pyspark.script.split_poi = $(proj.python.dir)/split_poi.py
python.script.ard_mapper = $(proj.python.dir)/ard_mapper_orc.py
python.script.ard_reducer = $(proj.python.dir)/ard_reducer_orc.py

#--------------
# Hive Server
#--------------
hiveserver.uri = jdbc:hive2://ec2-52-90-51-92.compute-1.amazonaws.com:10000

hiveserver.uri.embeded = jdbc:hive2://
#------------
# ARD Gen
#------------
ard.input.table = science_core_hrly
ard.tmp.table = xianglingmeng.ard_orc
ard.output.table = xianglingmeng.science_core_ex


ard.default.queue = user_freq
ard.default.tmp.schema = xianglingmeng.tmp
ard.default.join.schema = xianglingmeng.join

#-------------------
# Global Status Log
#-------------------
# MySQL
#status_log.db.conn.mysql.user     = $(xadcms.mysql.user)
#status_log.db.conn.mysql.password = $(xadcms.mysql.passwd)
#status_log.db.conn.mysql.host     = $(xadcms.mysql.master.host)
#status_log.db.conn.mysql.port     = $(xadcms.mysql.master.port)
#status_log.db.conn.mysql.dbname   = enigma_etl_hd2
#status_log.db.type  = mysql
#status_log.table    = status_log

#-------------------
# Local Status Log
#-------------------
status_log_local.table = status_log
status_log_local.db.type = mysql
status_log_local.db.conn.mysql.user = etl
status_log_local.db.conn.mysql.password = etlxaddb
status_log_local.db.conn.mysql.dbname = xad_etl
# SCI4
status_log_local.db.conn.mysql.host = db.ambari.mgmt.xad.com
status_log_local.db.conn.mysql.port = 3336

# MV1
#status_log_local.db.conn.mysql.host = nn02.corp.xad.com
#status_log_local.db.conn.mysql.port = 3306

# Status Log Keys
# houry key = science_core_x/us/exchange       2017 2 7 15
# daily key = science_core_x/us/exchange/DAILY 2017 2 7 NULL
status_log_local.key.science_core_x = science_core_x
status_log_local.tag.daily = DAILY

##---------------
## File Status
##---------------
proj.status.dir = $(proj.log.dir)/status

#------------
# S3
#------------
s3.access_key = AKIAJQ5GHKKUOYMQLZSQ
s3.secret_key = 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
hadoop.s3n = s3n
hadoop.s3a = s3a
hadoop.s3 = $(hadoop.s3n)

#-----------------
# Local Override
#-----------------
include = local-share.properties [optional]
include = local.properties [optional]
include = dev.properties [optional]

