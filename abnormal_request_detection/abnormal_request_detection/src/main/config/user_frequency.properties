#
# Insights ETL properties
#
# Copyright (C) 2016. xAd, Inc.  All Rights Reserved.
#

# ETL common properties is located in /home/xad/share/config
#include = etl_common.properties
include = command.properties

#------------------
# Review/Override
#------------------
#alert.email = tech-alerts@xad.com
alert.email = xiangling.meng@xad.com
alert.email.priority = 1

#---------
# Local
#---------
# Root Dirs
proj.root       = /home/xad
proj.name       = user_frequency
proj.home       = $(proj.root)/$(proj.name)
proj.lib.dir    = $(proj.home)/lib
proj.data.dir   = $(proj.home)/data
proj.bin.dir    = $(proj.home)/bin
proj.config.dir = $(proj.home)/config
proj.log.dir    = $(proj.home)/log
proj.log.pig.dir= $(proj.log.dir)/pig
proj.lock.dir   = $(proj.home)/lock
proj.pig.dir    = $(proj.home)/pig
proj.python.dir = $(proj.home)/python
proj.schema.dir = $(proj.home)/schema
proj.tmp.dir    = $(proj.home)/tmp
proj.var.dir    = $(proj.home)/var
proj.output.dir = $(proj.home)/output

# Files
lock.file = $(proj.lock.dir)/lock

# Folders
proj.folder.vis_hourly = vis_hourly
#proj.data.python.cache = $(proj.home)/data/python-cache.tgz

# Local - Proj.Data
proj.data.top_tsrc = $(proj.data.dir)/top_traffic_src.tsv
proj.data.top_sic  = $(proj.data.dir)/top_sic.tsv
proj.data.brands   = $(proj.data.dir)/brands.tsv

#----------
# Jars/Lib
#----------
# Shared Jars
share.java.dir  = $(proj.root)/share/java
jar.path.avro        = $(share.java.dir)/avro-1.7.6.jar
jar.path.json-simple = $(share.java.dir)/json-simple-1.1.jar
jar.path.xad-common  = $(share.java.dir)/xad_common.jar
jar.path.insights-etl= $(share.java.dir)/xad-insights-etl.jar
jar.path.user-frequency = $(share.java.dir)/xad-user_frequency.jar

#----------
# HDFS
#----------
# HDFS - Root
hdfs.root.local = $(proj.root)/hdfs
hdfs.root.cluster =
hdfs.root      = $(hdfs.root.cluster)
hdfs.data.dir  = $(hdfs.root)/data
hdfs.prod.dir  = $(hdfs.root)/prod
hdfs.tmp.dir   = $(hdfs.root)/tmp
hdfs.tmp.proj.dir = $(hdfs.tmp.dir)/$(proj.name)

# HDFS - Proj Dirs
hdfs.prod.dir         = $(hdfs.root)/prod
hdfs.proj.dir         = $(hdfs.prod.dir)/$(proj.name)
hdfs.proj.cache.dir   = $(hdfs.proj.dir)/cache
hdfs.proj.data.dir    = $(hdfs.proj.dir)/data
hdfs.proj.working.dir = $(hdfs.proj.dir)/working
hdfs.proj.tmp.dir     = $(hdfs.proj.dir)/tmp
hdfs.proj.output.dir  = $(hdfs.proj.dir)/output

hdfs.data.userhourly.dir = $(hdfs.data.dir)/user_frequency/user_hourly
hdfs.data.useraccum.dir = $(hdfs.data.dir)/user_frequency/user_accum


## HDFS - Proj.Cache Files
hdfs.proj.cache.top_tsrc = $(hdfs.proj.cache.dir)/top_traffic_src.tsv
hdfs.proj.cache.top_sic  = $(hdfs.proj.cache.dir)/top_sic.tsv
hdfs.proj.cache.brands    = $(hdfs.proj.cache.dir)/brands.tsv

# HDFS - Proj.Data Files
hdfs.proj.data.vis_hourly.dir = $(hdfs.proj.data.dir)/$(proj.folder.vis_hourly)

# HDFS - Redshift
hdfs.redshift.dir  = $(hdfs.data.dir)/redshift
hdfs.redshift.dimensions.dir  = $(hdfs.redshift.dir)/dimensions
hdfs.redshift.archive.dir = $(hdfs.redshift.dir)/archive
hdfs.redshift.traffic_src_dimension = $(hdfs.redshift.dimensions.dir)/traffic_src_dimension.tsv

#--------
# Pig
#--------
pig.exec.type = tez
pig.macros.enigma = $(proj.pig.dir)/enigma_macros.pig
pig.macros.insights_etl = $(proj.pig.dir)/insights_etl_macros.pig
pig.script.user_hourly = $(proj.pig.dir)/user_hourly.pig
pig.script.intermediate = $(proj.pig.dir)/intermediate.pig
pig.script.user_accum_blank= $(proj.pig.dir)/user_accum_blank.pig
#--------------------------
# Science Foundation Logs 
#--------------------------
sf.data.dir.hdfs = $(hdfs.data.dir)/extract
sf.data.dir.s3n = s3n://enigma-data-backup/extract
sf.data.dir.s3a = s3a://enigma-data-backup/extract
sf.data.dir.s3  = $(sf.data.dir.s3n)
sf.data.dir     = $(sf.data.dir.hdfs)

sf.countries = ca cn de es fr gb in jp us
sf.logtypes  = display exchange euwest1
sf.logtypes.cn = exchange cnnorth1
sf.logtypes.jp = display exchange euwest1 apnortheast1
sf.logtypes.us = display exchange euwest1
sf.data.window = L3

#-------------------------------
# Visition Hourly Summary Data
#-------------------------------
vs.data.dir.hdfs = $(hdfs.prod.dir)/insights-etl/data/vis_hourly
vs.data.dir = $(vs.data.dir.hdfs)


#-------------------------------
# User Frequency Hourly Summary Data
#-------------------------------
uf.data.dir.hdfs = $(hdfs.data.dir)/user_frequency
uf.data.useraccum.dir = $(uf.data.dir.hdfs)/user_hourly
uf.data.recentaccum.dir = $(uf.data.dir.hdfs)/recent_accum
uf.data.userhourly.dir = $(vs.data.dir.hdfs)
uf.data.blank.dir = $(uf.data.dir.hdfs)/accum.csv
uf.data.intermediate.dir =$(uf.data.dir.hdfs)/intermediate

#--------------
# Enigma Logs
#--------------
enigma.data.dir.hdfs = $(hdfs.data.dir)/enigma

#--------------------
# Visitation Hourly 
#--------------------
vhs.window = L5

vhs.tail.tsrc.id   = 999
vhs.tail.tsrc.name = trafficsrc_long_tail
vhs.tail.sic.id    = 999
vhs.tail.sic.name  = SIC_long_tail

# ETL Control
etl.missing.hour.threshold = 2

# Data retension (default in days)
vhs.keep.window.local  = 3
vhs.keep.window.hdfs   = 14
vhs.keep.window.s3     = 180
vhs.keep.window.status = 30
vhs.keep.window.db.month = 3


#---------------------------
# Dimensions
#---------------------------
# List of tables that can be automatically updated
dim.table.brand = footprints_brands_dimension
dim.table.sic   = footprints_sic_dimension
dim.table.pub = publisher_dimension
dim.tables = $(dim.table.brand) $(dim.table.sic) $(dim.table.pub)

# Primary key names
dim.primary_key.name.footprints_brands_dimension = footprints_brand_id
dim.primary_key.name.footprints_sic_dimension = footprints_sic_id
dim.primary_key.name.publisher_dimension = publisher_id

# Primary key positions
dim.primary_key.position = 0
dim.primary_key.position.footprints_brands_dimension = 0
dim.primary_key.position.footprints_sic_dimension = 0
dim.primary_key.position.publisher_dimension = 0

# Folders
dim.s3.src.dir = s3://enigma-dw2/dimensions
dim.s3.archive.dir = $(s3.dw.dim.dir)
dim.local.dir = $(proj.data.dir)/dimensions
dim.ext = .tsv

# Source Paths
dim.s3.src.path.footprints_brands_dimension = $(dim.s3.src.dir)/$(dim.table.brand).tsv
dim.s3.src.path.footprints_sic_dimension = $(dim.s3.src.dir)/$(dim.table.sic).tsv
dim.s3.src.path.publisher_dimension = $(dim.s3.src.dir)/$(dim.table.pub).tsv

# Schema
dim.schema.path.footprints_brands_dimension_new = $(proj.schema.dir)/$(dim.table.brand)_new.schema
dim.schema.path.footprints_sic_dimension_new = $(proj.schema.dir)/$(dim.table.sic)_new.schema
dim.schema.path.publisher_dimension_new = $(proj.schema.dir)/$(dim.table.pub)_new.schema

# Clean
dim.keep.window = 4


#---------------------------
# Data Warehouse (General)
#---------------------------
dw.table.vhs = visitation_hourly_summary

#-----------
# Redshift 
#-----------
redshift.conn.user     = root
redshift.conn.password = F**tpr1nt5
redshift.conn.host     = xad-userstore.cpjqerkiayhn.us-east-1.redshift.amazonaws.com
redshift.conn.port     = 5439
redshift.conn.dbname   = userstore

#-----
# S3
#-----
s3.dw.dir = s3://xad-science/dw
s3.dw.tmp.dir = s3://xad-science/dw/tmp
s3.dw.dim.dir = s3://xad-science/dw/dimensions
s3.dw.vis_hourly.dir = $(s3.dw.dir)/$(dw.table.vhs)
s3.dw.vis_hourly.tmp.dir = $(s3.dw.tmp.dir)/$(dw.table.vhs)

aws.access_key = AKIAJQ5GHKKUOYMQLZSQ
aws.secret_key = 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

#----------------
# Status Logs
#-----------------
# Local
proj.status.dir = $(proj.log.dir)/status
status.dir.vis_hourly = $(proj.status.dir)/$(proj.folder.vis_hourly)
status.dir.user_hourly = $(proj.status.dir)/user_hourly
status.dir.user_accum = $(proj.status.dir)/user_accum
status.dir.blank = $(proj.status.dir)/blank

# Enigma-ETL (in MySQL)
status_log.key.vhs.etl = /insights-etl/$(dw.table.vhs)
status_log.key.vhs.dw  = /insights-dw/$(dw.table.vhs)
status_log.key.science_foundation = /enigma/extract

status_log.db.type = mysql
status_log.table = status_log
status_log.db.conn.mysql.user = etl
status_log.db.conn.mysql.password = etlxaddb
#status_log.db.conn.mysql.host = etldb-ext.science.xad.com
status_log.db.conn.mysql.host = etldb.science.xad.com
status_log.db.conn.mysql.port = 3336
status_log.db.conn.mysql.dbname = enigma_etl

#-------------
# Distcp
#-------------
# Specify the maximum number of mappers; 0 means using the default.
distcp.max.maps = 5

#-------------
# Time Zones
#-------------
market.timezone    = America/New_York
market.timezone.ca = America/Toronto
market.timezone.cn = Asia/Shanghai
market.timezone.de = Europe/Berlin
market.timezone.es = Europe/Madrid
market.timezone.fr = Europe/Paris
market.timezone.gb = Europe/London
market.timezone.in = Asia/Kolkata
market.timezone.jp = Asia/Tokyo
market.timezone.us = America/New_York


#-------------
# Monitoring
#-------------
monitor.vhs.start_date = 2016/01/29

# Normal delay is 4 hours
monitor.vhs.max_delay = 10

#-------------
# P&G
#-------------
proj.folder.pg_gen_hourly = pg_gen_hourly
status.dir.pg_gen_hourly = $(proj.status.dir)/$(proj.folder.pg_gen_hourly)
status.dir.pg_join = $(proj.status.dir)/pg_join

pig.script.pg_gen   = $(proj.pig.dir)/pg_gen.pig
pig.script.pg_poi   = $(proj.pig.dir)/pg_poi.pig
pig.script.pg_join  = $(proj.pig.dir)/pg_join.pig

hdfs.proj.pg.dir         = $(hdfs.proj.dir)/pg
hdfs.proj.pg.extract.dir = $(hdfs.proj.pg.dir)/extract
hdfs.proj.pg.join.dir    = $(hdfs.proj.pg.dir)/join
hdfs.proj.pg.uid_path      = $(hdfs.proj.pg.dir)/pg_uid.tsv
hdfs.proj.pg.brand_path    = $(hdfs.proj.cache.brands)
hdfs.proj.pg.poi_orig.path = $(hdfs.proj.pg.dir)/pg_poi.tsv
hdfs.proj.pg.poi_norm.path = $(hdfs.proj.pg.dir)/poi_normalized


#-------------
# Twitter
#-------------

proj.folder.twitter_hourly = twitter_hourly
status.dir.twitter_hourly = $(proj.status.dir)/$(proj.folder.twitter_hourly)

pig.script.twitter_gen = $(proj.pig.dir)/twitter_gen.pig

hdfs.proj.twitter.dir = $(hdfs.proj.dir)/twitter
hdfs.proj.twitter.hourly.dir = $(hdfs.proj.twitter.dir)/hourly


#----------------
# Local Override
#-----------------
include = local.properties [optional]
include = dev.properties [optional]


