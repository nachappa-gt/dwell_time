#
# ard properties
#
# Copyright (C) 2014,2016 xAd, Inc.
#

include = command.properties

#------------------
# Review/Override
#------------------
alert.email = vimpy.batra@xad.com
alert.email.priority = 1

# process window: last two days
default.dates = L2-1

# number of mappers
distcp.max.maps = 6

#----------------
# DIRECTORIES
#----------------
proj.root       = /home/xad
proj.name       = ard
proj.home       = $(proj.root)/$(proj.name)
proj.lib.dir    = $(proj.home)/lib
proj.bin.dir    = $(proj.home)/bin
proj.config.dir = $(proj.home)/config
proj.log.dir    = $(proj.home)/log
proj.status.dir = $(proj.home)/status
proj.tmp.dir    = $(proj.home)/tmp
proj.lock.dir   = $(proj.home)/lock
proj.pig.dir    = $(proj.home)/pig
proj.python.dir = $(proj.home)/python

# Jars
share.java.dir  = $(proj.root)/share/java
jar.avro        = $(share.java.dir)/avro-1.7.6.jar
jar.json-simple = $(share.java.dir)/json-simple-1.1.jar
jar.parquet-pig-bundle = $(proj.lib.dir)/parquet-pig-bundle.jar
jar.xad.common  = $(share.java.dir)/xad_common.jar

# Lock
lock.file = $(proj.lock.dir)/lock

#----------------
# HDFS
#----------------
hdfs.root.local = $(proj.root)/hdfs
hdfs.root.cluster =
hdfs.root      = $(hdfs.root.local)
hdfs.share.dir = $(hdfs.root)/share
hdfs.data.dir  = $(hdfs.root)/data
hdfs.tmp.dir   = /tmp/ard


#----------------------------
# Default/Common Properties
#-----------------------------
default.countries   = us gb in de ca fr cn it es jp
default.logtypes    = display exchange euwest1
default.logtypes.cn = cnnorth1
default.logtypes.us = display exchange euwest1 search
default.logtypes.jp = display exchange euwest1 apnortheast1

# Start dates
default.date.format   = yyyy/MM/dd
country.start.date    = 2013/11/27
country.start.date.de = 2014/02/26
country.start.date.ca = 2014/02/26
country.start.date.cn = 2014/09/02
country.start.date.fr = 2014/09/10
country.start.date.it = 2015/05/15
country.start.date.es = 2015/05/15
event.start.date.AdUserProfile = 2014/02/06
event.start.date.HttpVendorStats = 2014/07/01

#---------------------
# HE Colo (PnP Colo)
#---------------------
remote.hadoop.host  = sci07
remote.hadoop.port  = 50070
remote.hadoop.proto = hftp
remote.hadoop.bin.dir  = /usr/bin
remote.hadoop.conf.dir = /etc/hadoop/conf.he2

#--------------------
# Common Properties
#--------------------
ard.file.success = _SUCCESS

#--------------------
# Enigma Properties
#--------------------
# Directories (aggregation started 2014/03/25)
enigma.data.prefix.s3.orig   = s3://enigma-data/raw-data/camus/data
enigma.data.prefix.s3.backup = s3://enigma-data-backup/raw-data/camus/data
enigma.data.prefix.s3 = $(enigma.data.prefix.s3.backup)
enigma.data.prefix.s3.AdUserProfile = $(enigma.data.prefix.s3.orig)
enigma.data.prefix.s3.HttpVendorStats = $(enigma.data.prefix.s3.orig)
enigma.data.prefix.hdfs = /data/enigma

# Events
# enigma.events = AdRequest AdDetails AdTracking AdUserProfile HttpVendorStats
enigma.events = AdRequest AdDetails AdTracking AdUserProfile
enigma.event.folder.prefix = enigma_event_
enigma.hourly.folder = hourly
enigma.dates   = L3
enigma.status_log.prefix = raw_data_backup-
enigma.log.dir = $(proj.log.dir)/enigma
enigma.keep.window = 95
enigma.keep.window.AdUserProfile = 120
enigma.keep.window.nofill = 65

#--------------------------
# Publisher ETL Properties
#--------------------------
pub.data.prefix.s3 = s3://xad-science/forecast_etl/data
pub.data.prefix.hdfs = /data/forecast_etl
pub.events      = hourly_summary
pub.keep.window = 30
pub.dates       = L2-1
pub.tmp.dir     = ${hdfs.tmp.dir}

#--------------------
# XADCMS Properties
#--------------------
# DB
xadcms.mysql.master.host = etldb.science.xad.com
xadcms.mysql.master.host.ext = etldb-ext.science.xad.com
xadcms.mysql.master.port = 3336
xadcms.mysql.slave.host  = db01.internal.xad.com
xadcms.mysql.slave.port  = 3306
xadcms.mysql.staging.host  = staging.xadcentral.com
xadcms.mysql.staging.port  = 3336
xadcms.mysql.host   = $(xadcms.mysql.master.host)
xadcms.mysql.port   = $(xadcms.mysql.master.port)
xadcms.mysql.user   = etl
xadcms.mysql.passwd = etlxaddb
xadcms.mysql.dbname = xadcms
xadcms.mysql.tables = term mslocation mobilesite account adgroup adgrouplocation budget budget_enigma campaign category company creative keywords publisher publishersettings targetprofile tierbidsettings tiersettings
xadcms.data.prefix.hdfs = /data/xadcms
xadcms.tmp.prefix = XADCMS_
xadcms.date.format  = yyyy-MM-dd
xadcms.keep.window  = 7
xadcms.data.format  = avro
xadcms.data.formats = csv avro

# Sqoop options
sqoop.import.option.xadcms.account = --split-by id
sqoop.import.option.xadcms.adgrouplocation = --split-by id
sqoop.import.option.xadcms.adgroup = --split-by id
sqoop.import.option.xadcms.budget_enigma = --split-by id
sqoop.import.option.xadcms.budget = --split-by id
sqoop.import.option.xadcms.campaign = --split-by id
sqoop.import.option.xadcms.category = --split-by id
sqoop.import.option.xadcms.company = --split-by id
sqoop.import.option.xadcms.creative = --split-by id
sqoop.import.option.xadcms.keywords = --split-by id
sqoop.import.option.xadcms.mobilesite = --split-by id
sqoop.import.option.xadcms.mslocation = --split-by id
sqoop.import.option.xadcms.publisher = --split-by id
sqoop.import.option.xadcms.publishersettings = --split-by id
sqoop.import.option.xadcms.targetprofile = --split-by id
sqoop.import.option.xadcms.term = -m 4 --split-by id
sqoop.import.option.xadcms.tierbidsettings = --split-by id
sqoop.import.option.xadcms.tiersettings = --split-by id

#--------------------
# POI DB Properties
#--------------------
poidb.conn.host = poi-db.mgmt.xad.com
poidb.conn.port = 5432
poidb.conn.user   = ops
poidb.conn.passwd = x@d0P55
poidb.conn.dbname = xaddb
poidb.tables = poi
poidb.tables.split = poi
poidb.data.prefix.hdfs = /data/poidb
poidb.tmp.prefix = POIDB
poidb.date.format  = yyyy-MM-dd
poidb.keep.window  = 7
poidb.data.format.sqoop = avro
poidb.data.format.split = parquet

# sqoop.import.option.xaddb.poi = --split-by id --columns id,\"hashKey\",\"companyName\",latitude,longitude,address,city,state,zip,country,dma_code,flagged,source_id,del
sqoop.import.option.xaddb.poi = --split-by id --columns id,\"hashKey\",\"companyName\",latitude,longitude,city,state,zip,country,dma_code,\"brandId\",flagged,reason,hide,del

#---------
# Python
#---------
python.cmd = /opt/anaconda/bin/python2.7
pyspark.script.split_poi = $(proj.python.dir)/split_poi.py

#--------
# Spark
#--------
spark.driver.memory = 2G
spark.executor.memory = 4G
spark.num.executors = 8

#-----------------------------
# call_tracking_log
#-----------------------------
ctl.table.name   = call_tracking_log
ctl.status_log.key = /enigma/advreport/call_tracking_summary/us
# Local logs
ctl.log.dir      = $(proj.log.dir)/call_tracking_log
ctl.log.dir.s3   = $(ctl.log.dir)/s3
ctl.log.dir.hdfs = $(ctl.log.dir)/hdfs
# Data Prefix
ctl.data.prefix.s3   = s3://enigma-dw2/telmetrics/call_tracking_log
ctl.data.prefix.hdfs = /data/redshift/call_tracking_log
ctl.file.prefix.s3   = part-r-00
# Download windows
ctl.keep.window  = 80
ctl.dates        = L3-1

#-----------------------------
# Campaign Hourly Summary
#-----------------------------
chs.data.prefix.s3   = s3://enigma-dw2/facts/campaign_hourly_summary
chs.data.prefix.hdfs = /data/redshift/campaign_hourly_summary
chs.status_log.prefix = /enigma/advreport/campaign_hourly_summary
chs.log.dir = $(proj.log.dir)/campaign_houly_summary
chs.dates = L3
chs.keep.window = 65

#-----------------------------
# RTB Hourly Summary
#-----------------------------
rhs.data.prefix.s3   = s3://enigma-dw2/facts/rtb_hourly_summary
rhs.data.prefix.hdfs = /data/redshift/rtb_hourly_summary
rhs.status_log.prefix = /enigma/rtbreport/rtb_hourly_summary
rhs.log.dir = $(proj.log.dir)/rtb_houly_summary
rhs.dates = L3
rhs.keep.window = 35

#-------------------------
# RedShift DB - dwenigma
#-------------------------
redshift.dimension.hdfs.dir = /data/redshift/dimensions
redshift.archive.dir = /data/redshift/archive
redshift.archive.keep.window = 3

# Tables
dwenigma.dimension.tables = traffic_src_dimension top_traffic_src_dimension footprints_sic_dimension campaign_dimension
userstore.dimension.tables =
xadcms.dimension.tables = country_timezone

# Connections 
dwenigma.conn.host     = redshift.xad.com
dwenigma.conn.port     = 5439
dwenigma.conn.user     = root
dwenigma.conn.password = xAd3n1gMa
dwenigma.conn.dbname   = dwenigma

userstore.conn.host     = xad-userstore.cpjqerkiayhn.us-east-1.redshift.amazonaws.com
userstore.conn.port     = 5439
userstore.conn.user     = root
userstore.conn.password = F**tpr1nt5
userstore.conn.dbname   = userstore

#---------------------
# Science Core (AVRO)
#---------------------
# Data location in S3
extract.data.prefix.s3 = s3://enigma-data-backup/extract
# Data location in HDFS
extract.data.prefix.hdfs = /data/extract
# MysQL status prefix for logs in S3
extract.status_log.prefix = /enigma/extract
# MysQL status prefix for logs in HDFS
extract.download.status_log.prefix = sci3:science_core_avro
extract.dates = L1
extract.keep.window = 60
extract.keep.window.nofill = 60
extract.log.dir = $(proj.log.dir)/extract

#--------------------------------
# Science Core Conversion (TEST)
#--------------------------------
science_core.format = parquet
science_core.parquet.data.prefix.hdfs = /data/science_core_parquet
science_core.parquet.status.dir = $(proj.status.dir)/science_core/parquet
science_core.fill.folders = fill,nf
science_core.sl.folders = tll,pos,rest

# Pig
proj.log.pig.dir = $(proj.log.dir)/pig
pig.script.science_core_parquet = $(proj.pig.dir)/science_core_parquet.pig


#------------
# Status Log
#------------
# MySQL
status_log.db.conn.mysql.user     = $(xadcms.mysql.user)
status_log.db.conn.mysql.password = $(xadcms.mysql.passwd)
status_log.db.conn.mysql.host     = $(xadcms.mysql.master.host)
status_log.db.conn.mysql.port     = $(xadcms.mysql.master.port)
status_log.db.conn.mysql.dbname   = enigma_etl_hd2
status_log.db.type  = mysql
status_log.table    = status_log

## Local
proj.status.dir = $(proj.log.dir)/status

#------------
# S3
#------------
s3.access_key = AKIAJQ5GHKKUOYMQLZSQ
s3.secret_key = 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
hadoop.s3n = s3n
hadoop.s3a = s3a
hadoop.s3 = $(hadoop.s3n)

#-----------------
# Tmp Cleanning
#-----------------
clean.hdfs.tmp.keep.window = 7
clean.hdfs.tmp.dir = /tmp
clean.hdfs.sub.tmp.patterns = hive- ard-
clean.hdfs.user.tmp.folders = .Trash .staging

#-----------------
# Local Override
#-----------------
include = local-share.properties [optional]
include = local.properties [optional]
include = dev.properties [optional]

