#!/usr/bin/env python2.7

## This is used to build the pipeline for abnormal request detection

import sys, os
import time
import datetime
import logging
import logging.config
import logging.handlers as handlers
import getopt, re
from string import Template
import subprocess
import ConfigParser
import shlex
import time
import socket

sys.path.append('/home/xad/sar-optimization/lib/modules/cmnfunc')
import cmnfunc

gl_start_date_str = '2013-12-31'
gl_end_date_str = time.strftime("%Y-%m-%d")
gl_table_names = ['science_core_hrly'];
gl_countries = ['us']
gl_fills =['nf', 'fill']
gl_loc_scores=['tll', 'pos', 'rest']
gl_update_schema=0
gl_cmd_flag = 0
gl_drop_before_add=0
gl_country_prod_type = {'default':'display,exchange,euwest1'}
gl_allowed_table_names = ['science_core_hrly','ex_HttpVendorStats','science_core_orc', 'ex_AdDetails' ,'ex_AdTracking','ex_HttpVendorStats']

algo_logger = logging.getLogger('')
schema_host_type = 'http'

schema_host = "gw01.internal.xad.com" # he2
schema_host = "172.17.19.56" # sci2

raw_data_storage_type='s3'
raw_data_dir_s3 = 's3n://enigma-data-backup/extract'
raw_data_dir_hdfs = '/data/extract'
raw_data_dir_backup_s3 = 's3n://enigma-data/raw-data/camus/data'
gl_raw_data_dir = raw_data_dir_hdfs

def check_source(base_path, country, prod_type, cluster = None):
    ''' Can be used for:
            1. When cluster != None, check the lastest available hdfs data based on xcp log (on other gateway).
               return the lastest finished hdfs hour
            2. When cluster==None, check local log for the last finished hive import hour
    '''
    if cluster!=None:
        cmd = 'ssh ' + cluster + ' ls '+ base_path
    else:
        cmd = 'ls '+ base_path
    cmd = cmd + country + '_' + prod_type + '/'
    lastest_hour = ''

    for it in range(4): #find the largest year, month, day, hour in log
        out_sys = os.popen(cmd).read()
        last_one = lastest(out_sys)
        if last_one == '': # there is no folder/file under this directory
            return -1
        lastest_hour += last_one
        cmd = cmd + last_one + '/'

    return lastest_hour

def lastest(osout):
        ''' input: stdout of 'ls' in current level
            output: The largest number among output numbers '''
        allhours = filter(lambda ih: ih.isdigit(), osout.split('\n')) #only keep numeric output
        return max(allhours)


def hrly_xcp_data_exist(table_name, curr_hour, prod_type, country):
    ''' Function used to ensure the xcp data for each hour exist. '''
    xcp_log_base_path = gl_log_path['xcp_log']
    cmd = 'test -d ' + xcp_log_base_path + country + '_' + prod_type + '/'
    cmd = cmd + datetime.datetime.strftime(curr_hour, '%Y/%m/%d/%H')
    if os.system(cmd) == 0:
        return True
    elif gl_cmd_flag==1:
        return True
    else:
        algo_logger.error('Input xcp missing data in the middle of the timeframe' + cmd)
        return False


def get_avro_file ( path ):
    shell_cmd = "hadoop fs -ls %s/*.avro" % path
    cmd =["-c", shell_cmd]
    rc, stdout, stderr = cmnfunc.run_cmd_full(cmd)
    if( rc!=0 ):
        return None
    
    sp = stdout.split('\n')
    if len(sp)<=0 :
        return None
    
    row = sp[0]
    file  = row.split(' ')[-1]
    print file
    return file



def run_hive_cmd_file( output_path, post_cmd = None, queue_size=1, fcmd = None):
    global job_queue
    #cmd = ["hive", "-f", output_path]
    cmd = ["beeline", "-n", "xad", "-u",  "\"jdbc:hive2://ip-172-17-25-136.ec2.internal:2181,ip-172-17-25-137.ec2.internal:2181,ip-172-17-25-135.ec2.internal:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2\"", "-f", output_path]
    #cmd = ["/usr/hdp/current/hive-server2-hive2/bin/beeline", "-u",  "jdbc:hive2://ip-172-17-25-135.ec2.internal:10500/default", "-f", output_path] 

    algo_logger.info(" ".join(cmd))
    sys.stdout.flush()
    sys.stderr.flush()

    try:
        job_queue
    except NameError:
        job_queue=[]
    while True:
        if len(job_queue) < queue_size:
            #return #not catching the stdout anymore
            p = subprocess.Popen(cmd)
            job_queue.append((p, post_cmd, fcmd) )
            time.sleep(10) # sleep 20s to avoid too frequent hive -f submit. If not sleep, will cause mysterious error
            break
        time.sleep(2)
        for ijob in job_queue:
            if ijob[0].poll() is None:
                continue
            if ijob[0].poll() == 0:
                if ijob[1] is not None:
                    os.system(ijob[1])
                job_queue.remove(ijob)
            else:
                job_queue.remove(ijob)
                if ijob[2] is None:
                    returninfo = ijob[1] if ijob[1] is not None else ''
                    raise Exception('Error while executing hive cmd: ', returninfo + '\n' + 'return code: ' + str(ijob[0].poll()))  
                else: #retry if the first attemp failed. Something hive -f filename will send incomplete file.
                    algo_logger.warning('First Attemp failed for job: %s. Return code: %d. Code to retry:\n %s' %(ijob[1], ijob[0].poll(), ijob[2]) )
                    f_retry = '/home/xad/sci_hive_warehouse/tmp/cmd_retry.sql' + str(datetime.datetime.now().minute) + str(datetime.datetime.now().second)
                    with open(f_retry, 'w') as ffcmd:
                        ffcmd.write(ijob[2])
                    run_hive_cmd_file(f_retry, ijob[1], queue_size)



def empty_job_queue():
    ''' function to ensure all the hive jobs are finished'''
    global job_queue
    while len(job_queue) > 0:
        for ijob in job_queue:
            if ijob[0].poll() == None:
                continue
            if ijob[0].poll() == 0:
                if ijob[1] != None:
                    os.system(ijob[1])
                job_queue.remove(ijob)
            else:
                raise Exception('Error while executing hive cmd: ', ijob[1])  





































